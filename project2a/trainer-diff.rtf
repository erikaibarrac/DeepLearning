{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \CocoaLigature0 Last login: Tue Nov 14 20:54:07 on ttys002\
erikaibarra@MacBook-Pro-368 ~ % cd Desktop\
erikaibarra@MacBook-Pro-368 Desktop %  diff trainer_actual.py minGPT/mingpt/trainer.py              \
11a12\
> import numpy as np\
76c77\
< \
---\
>         ################################### Get self parameters #######################################################\
78c79,82\
<         self.iter_num = 0\
---\
>         self.iter_num = model.iter_num if hasattr(model, 'iter_num') else 0  \
>         self.iter_list = model.iter_list if hasattr(model, 'iter_list') else []  \
>         self.since_last_save = 0  \
>         self.checkpoint_num = model.checkpoint_num if hasattr(model, 'checkpoint_num') else 0   \
79a84\
>         self.saved_loss = model.saved_loss if hasattr(model, 'saved_loss') else []  \
80a86,91\
>         checkpoint_name = config.checkpoint_name if hasattr(config, 'checkpoint_name') else 'checkpoint'  \
> \
>         ##################################### Get the loss ################################################################\
>         self.loss = self.saved_loss[-1] if self.saved_loss else np.inf \
>         self.curr_loss = []\
> \
90a102,103\
>             x = x.squeeze(0)  \
>             y = y.squeeze(0)  \
91a105,107\
>     \
>             prev_loss = self.loss  # Get loss before\
> \
93a110\
>             self.curr_loss.append(self.loss.detach())\
105a123,125\
>             \
>     \
>             self.since_last_save += 1 \
106a127,145\
>             ##################################### A change for getting checkpoints ############################################################\
>             if self.loss <= prev_loss and self.since_last_save >= config.checkpoint_iters:\
>                 self.since_last_save = 0\
>                 \
>                 # Create and save our checkpoint\
>                 checkpoint = \{\
>                     'model_transformer': model.transformer.state_dict(),\
>                     'model_lm_head': model.lm_head.state_dict(),\
>                     'optimizer_state_dict': self.optimizer.state_dict(),\
>                     'loss': self.loss,\
>                     'iter_num': self.iter_num,\
>                     'iter_list': self.iter_list.append(self.iter_num),\
>                     'checkpoint_num': self.checkpoint_num,\
>                     'saved_loss': self.saved_loss.append(self.loss)\
>                 \}\
>                 torch.save(checkpoint, f'checkpoints/\{checkpoint_name\}_\{self.checkpoint_num\}.pth')\
>                 self.checkpoint_num += 1\
>                 \
> \
erikaibarra@MacBook-Pro-368 Desktop % }